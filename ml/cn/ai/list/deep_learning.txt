rnn/lstm/gru/bi-lstm

bi-lstm实现序列标注

bi-lstm + crf

word2vector:
cbow:context->word
skip-gram:word->context
NEC
Negative sampling（负采样）
Hierarchical softmax（层次softmax）
huffman树：每次选取n个结点中权值最小的两个结点，同级节点左小右大

seq2seq和attention机制
https://cloud.tencent.com/developer/news/372017
https://blog.csdn.net/Mbx8X9u/article/details/79908973
http://fancyerii.github.io/2019/03/09/transformer-illustrated/#self-attention%E7%AE%80%E4%BB%8B(Multi-Head Attention;位置编码;残差连接等)
https://blog.csdn.net/fan_fan_feng/article/details/81666736 (描述的更清楚一些)

attention-based lstm (at-lstm)
基于注意力的lstm模型

全连接网络


FastText:
fastText是一个快速文本分类算法，与基于神经网络的分类算法相比有两大优点：
1、fastText在保持高精度的情况下加快了训练速度和测试速度
2、fastText不需要预训练好的词向量，fastText会自己训练词向量
3、fastText两个重要的优化：Hierarchical Softmax、N-gram



glove

ELMo
lstm双层双向

GBT
Transform单向


Transformer模型
http://fancyerii.github.io/2019/03/09/transformer-illustrated/
https://baijiahao.baidu.com/s?id=1622064575970777188&wfr=spider&for=pc(最形象的直白细节展示)
https://www.jianshu.com/p/ef41302edeef
论文：https://arxiv.org/pdf/1706.03762.pdf  （attention is all your need）

Bert模型
https://zhuanlan.zhihu.com/p/47488095
https://cloud.tencent.com/developer/article/1389555 （生动直白版）
bert的位置嵌入是学习出来的，词嵌入可以学习也可以固定
分割嵌入（Segment Embedding）：根据模型学习出来的

Batch Normalization
https://www.cnblogs.com/skyfsm/p/8453498.html

语言模型
http://fancyerii.github.io/books/lm/



人脸检测 mtcnn模型
NMS(非极大值抑制)

FCN全卷积网络

