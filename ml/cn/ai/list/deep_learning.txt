rnn/lstm/gru


word2vector:
cbow:context->word
skip-gram:word->context
NEC
Negative sampling（负采样）
Hierarchical softmax（层次softmax）

seq2seq和attention机制
https://cloud.tencent.com/developer/news/372017
https://blog.csdn.net/Mbx8X9u/article/details/79908973
http://fancyerii.github.io/2019/03/09/transformer-illustrated/#self-attention%E7%AE%80%E4%BB%8B(Multi-Head Attention;位置编码;残差连接等)
https://blog.csdn.net/fan_fan_feng/article/details/81666736 (描述的更清楚一些)

attention-based lstm (at-lstm)
基于注意力的lstm模型

全连接网络

FastText


ELMo

GBT

Transformer模型
http://fancyerii.github.io/2019/03/09/transformer-illustrated/
https://baijiahao.baidu.com/s?id=1622064575970777188&wfr=spider&for=pc(最形象的直白细节展示)
https://www.jianshu.com/p/ef41302edeef
论文：https://arxiv.org/pdf/1706.03762.pdf  （attention is all your need）

Bert模型
https://zhuanlan.zhihu.com/p/47488095

Batch Normalization
https://www.cnblogs.com/skyfsm/p/8453498.html

语言模型
http://fancyerii.github.io/books/lm/



人脸检测 mtcnn模型
NMS(非极大值抑制)

FCN全卷积网络

