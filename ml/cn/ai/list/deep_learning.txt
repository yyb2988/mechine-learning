rnn/lstm/gru


seq2seq和attention机制
https://cloud.tencent.com/developer/news/372017
https://blog.csdn.net/Mbx8X9u/article/details/79908973
http://fancyerii.github.io/2019/03/09/transformer-illustrated/#self-attention%E7%AE%80%E4%BB%8B(Multi-Head Attention;位置编码;残差连接等)
https://blog.csdn.net/fan_fan_feng/article/details/81666736 (描述的更清楚一些)

attention-based lstm (at-lstm)
基于注意力的lstm模型

Transformer模型
http://fancyerii.github.io/2019/03/09/transformer-illustrated/
https://www.jianshu.com/p/ef41302edeef
论文：https://arxiv.org/pdf/1706.03762.pdf  （attention is all your need）

Batch Normalization
https://www.cnblogs.com/skyfsm/p/8453498.html

语言模型
http://fancyerii.github.io/books/lm/

人脸检测 mtcnn模型
NMS(非极大值抑制)

FCN全卷积网络

