深度学习相关文本分类：
https://www.cnblogs.com/jiangxinyang/p/10208227.html


rnn/lstm/gru/bi-lstm

gru:zt和rt分别表示更新门和重置门,没有细胞状态Ct

bi-lstm实现序列标注
捕获双向的语义依赖，隐向量拼接

bi-lstm + crf

word2vector:
cbow:context->word
skip-gram:word->context
NEC
Negative sampling（负采样）
Hierarchical softmax（层次softmax）
huffman树：每次选取n个结点中权值最小的两个结点，同级节点左小右大

seq2seq和attention机制
https://cloud.tencent.com/developer/news/372017
https://blog.csdn.net/Mbx8X9u/article/details/79908973
http://fancyerii.github.io/2019/03/09/transformer-illustrated/#self-attention%E7%AE%80%E4%BB%8B(Multi-Head Attention;位置编码;残差连接等)
https://blog.csdn.net/fan_fan_feng/article/details/81666736 (描述的更清楚一些)

attention-based lstm (at-lstm)
基于注意力的lstm模型

全连接网络


BatchNormalization:
批标准化(每个神经元需要训练均值,方差,伽马,贝塔四个参数)
（1）神经网络本质是学习数据分布，如果寻来你数据与测试数据分布不同，网络的泛化能力将降低，batchnorm就是通过对每一层的计算做scale和shift的方法，通过规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到正太分布，减小其影响，让模型更加健壮。
（2）使用BN面向梯度消失等，去加速网络收敛加快训练速度。
(3) 防止过拟合。此时可以移除或使用较低的dropout，降低L2权重衰减系数等防止过拟合的手段。
https://www.cnblogs.com/skyfsm/p/8453498.html
移动平均法求全局的均值,方差用于预测阶段。
指数加权平均、偏差修正：
https://www.cnblogs.com/guoyaohua/p/8544835.html




影子变量

FastText:
fastText是一个快速文本分类算法，与基于神经网络的分类算法相比有两大优点：
1、fastText在保持高精度的情况下加快了训练速度和测试速度
2、fastText不需要预训练好的词向量，fastText会自己训练词向量
3、fastText两个重要的优化：Hierarchical Softmax、N-gram

GlobalAveragePooling1D、GlobalMaxPooling1D： <none,12,512> => <none,512>
keras.layers.MaxPooling1D: <none,12,512> => <none,2,512>

glove

ELMo:
https://www.cnblogs.com/huangyc/p/9860430.html
https://blog.csdn.net/u013166817/article/details/85376644
先用其他大型语料pretrain好ELMO词向量，再用该task的语料fine tuningELMO词向量，然后固定该ELMO词向量的参数，将ELMO词向量 concat 原始词向量后作为task model的输入，在这个过程中不断的学习这个线性权重参数。部分task中是将ELMO词向量用在输出那部分的，是将ELMO词向量 concat task model最后的hidden state，其实就是作为最后全连接层的输入，也是不断的学习sj这个线性权重参数。
lstm双层双向
词向量是深度双向语言模型(biLM)内部状态的函数，在一个大型文本语料库中预训练而成。
说到词向量，我们一定会联想到word2vec，因为在它提出的词向量概念给NLP的发展带来了巨大的提升。而ELMo的主要做法是先训练一个完整的语言模型，再用这个语言模型去处理需要训练的文本，生成相应的词向量，所以在文中一直强调ELMo的模型对同一个字在不同句子中能生成不同的词向量
预训练的目标函数: 就是取这两个方向语言模型的最大似然
大家有想过为什么ELMo的效果会比word2vec的效果好？我个人认为有一下几点：
ELMo的假设前提一个词的词向量不应该是固定的，所以在一词多意方面ELMo的效果一定比word2vec要好。
word2vec的学习词向量的过程是通过中心词的上下窗口去学习，学习的范围太小了，而ELMo在学习语言模型的时候是从整个语料库去学习的，而后再通过语言模型生成的词向量就相当于基于整个语料库学习的词向量，更加准确代表一个词的意思。
ELMo还有一个优势，就是它建立语言模型的时候，可以运用非任务的超大语料库去学习，一旦学习好了，可以平行的运用到相似问题。



GBT
Transform单向
他们利用了Transformer网络代替了LSTM作为语言模型来更好的捕获长距离语言结构。然后在进行具体任务有监督微调时使用了语言模型作为附属任务训练目标
Transformer解码器作为语言模型。Transformer模型主要是利用自注意力（self-attention）机制的模型
然后再具体NLP任务有监督微调时，与ELMo当成特征的做法不同，OpenAI GPT不需要再重新对任务构建新的模型结构，而是直接在transformer这个语言模型上的最后一层接上softmax作为任务输出层，然后再对这整个模型进行微调。他们额外发现，如果使用语言模型作为辅助任务，能够提升有监督模型的泛化能力，并且能够加速收敛。

Transformer模型
http://fancyerii.github.io/2019/03/09/transformer-illustrated/
https://baijiahao.baidu.com/s?id=1622064575970777188&wfr=spider&for=pc(最形象的直白细节展示)
https://www.jianshu.com/p/ef41302edeef
论文：https://arxiv.org/pdf/1706.03762.pdf  （attention is all your need）

Bert模型
https://zhuanlan.zhihu.com/p/47488095
https://cloud.tencent.com/developer/article/1389555 （生动直白版）
bert的位置嵌入是学习出来的，词嵌入可以学习也可以固定
分割嵌入（Segment Embedding）：根据模型学习出来的


对比一下三种语言模型结构，BERT使用的是Transformer编码器，由于self-attention机制，所以模型上下层直接全部互相连接的。而OpenAI GPT使用的是Transformer解码器，它是一个需要从左到右的受限制的Transformer，而ELMo使用的是双向LSTM，虽然是双向的，但是也只是在两个单向的LSTM的最高层进行简单的拼接。所以作者们任务只有BERT是真正在模型所有层中是双向的。


Batch Normalization
https://www.cnblogs.com/skyfsm/p/8453498.html

语言模型
http://fancyerii.github.io/books/lm/



人脸检测 mtcnn模型
NMS(非极大值抑制)

FCN全卷积网络

