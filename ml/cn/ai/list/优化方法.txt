1.批量梯度下降(BGD),随机梯度下降(SGD),小批量梯度下降(MBGD)：
https://www.cnblogs.com/lliuye/p/9451903.html
梯度下降为什么陡:
https://www.zhihu.com/question/36301367/answer/198887937
梯度下降的方向取决于当前批次或个体

2.momentum:动量
它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向
本次校准后梯度：(η*当前损失函数梯度 - a*上次校准后梯度) 艾塔
w = w - 本次校准后梯度

3.Adagrad：(adaptive gradient)自适应梯度算法,是一种改进的随机梯度下降算法
g:本次迭代中损失函数L对0的梯度
累计梯度平方r = g*g
0 = 0 -  g * η   /   a+(r开方) [a是为了防止分母为0]
j维度更新次数越多,学习率越衰减;之前更新幅度越大,学习率越衰减
https://blog.csdn.net/program_developer/article/details/80756008

牛顿法:求解无约束最优化
牛顿法的核心思想是对函数进行泰勒展开。泰勒公式：f(x) = f(a) + (x-a)*f`(a)/1! + (x-a)2 * f``(a) /2! +  (x-a)3 * f```(a) / 3!
牛顿法的用处： 求解方程的根，用一阶泰勒展开；用于凸优化，用二阶泰勒展开，转化为求一阶导数函数的根
https://yq.aliyun.com/articles/655109
https://blog.csdn.net/lilong117194/article/details/78111779
作为优化方法时,利用了二阶导数的信息,因此下降更快。
牛顿法的优缺点总结：
　　优点：二阶收敛，收敛速度快；
　　缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵(二阶导逆矩阵)，计算比较复杂。H矩阵半正定,可能没有逆,求解过程无法持续。
XGBoost本质上就是利用牛顿法进行优化的


拟牛顿法:求解无约束最优化。常用的拟牛顿法有DFP算法和BFGS算法。
拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。

凸优化
http://www.cnblogs.com/tornadomeet/p/3300132.html
凸集，凸函数，凸优化问题，线性规划，二次规划，二次约束二次规划，半正定规划
凸优化定义：目标函数为凸函数,不等式函数为凸函数，等式函数为仿射函数。




梯度下降 ,最小二乘法/牛顿法/拟牛顿法/共轭梯度法/GIS/IIS
  协方差矩阵
