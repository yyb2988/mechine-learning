1.批量梯度下降(BGD),随机梯度下降(SGD),小批量梯度下降(MBGD)：
https://www.cnblogs.com/lliuye/p/9451903.html
梯度下降为什么陡:
https://www.zhihu.com/question/36301367/answer/198887937
梯度下降的方向取决于当前批次或个体

2.momentum:动量
它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向
本次校准后梯度：(η*当前损失函数梯度 - a*上次校准后梯度) 艾塔
w = w - 本次校准后梯度
或者这么表示：
v = mu * v - learning_rate * dw
w = w + v
其中，v初始化为0，mu是设定的一个超变量，最常见的设定值是0.9。可以这样理解上式：如果上次的momentum()与这次的
负梯度方向是相同的，那这次下降的幅度就会加大，从而加速收敛。



3.Adagrad：(adaptive gradient)自适应梯度算法,是一种改进的随机梯度下降算法
g:本次迭代中损失函数L对0的梯度
累计梯度平方r(t) = r(t-1) + g*g
0 = 0 -  g * η   /   a+(r开方) [a是为了防止分母为0]
j维度更新次数越多,学习率越衰减;之前更新幅度越大,学习率越衰减
https://blog.csdn.net/program_developer/article/details/80756008
缺点：学习率衰减过快,用RMSprop,AdaDelta方法解决

4.RMSprop
对adagrad的改进在于学习率的衰减不再对历史上所有梯度平方累计，而是做加权平均
累计加权梯度平方 r(t) = γ * r(t-1)  +  (1-γ)*g*g   γ：gamma
梯度按元素平方做指数加权移动平均,梯度不再一直降低或者不变
建议设定 γ 为 0.9, 学习率 η 为 0.001



5.AdaDelta



6.adam
https://www.cnblogs.com/yifdu25/p/8183587.html
指数加权平均,相当于是最近 1/(1-β)天的平均值,因此才有误差修正: https://blog.csdn.net/aliceyangxi1987/article/details/81427631
方向和学习同时校正,结合了RMSprop和momentum的优点,相当于 RMSprop + Momentum
是实际学习中最常用的算法,也是效果最好的算法
建议 β1 ＝ 0.9，β2 ＝ 0.999，ϵ ＝ 10e−8
β1是Momentum参数：
m(t) = β1 * m(t-1) + (1-β1) * g
β2是RMSprop 的 γ参数：
r(t) = β2 * r(t-1)  +  (1-β2)*g*g   
0 = 0 -  m(t) * η   /   ϵ+(r开方) [ϵ是为了防止分母为0]


牛顿法:求解无约束最优化
牛顿法的核心思想是对函数进行泰勒展开。泰勒公式：f(x) = f(a) + (x-a)*f`(a)/1! + (x-a)2 * f``(a) /2! +  (x-a)3 * f```(a) / 3!
牛顿法的用处： 求解方程的根，用一阶泰勒展开；用于凸优化，用二阶泰勒展开，转化为求一阶导数函数的根
https://yq.aliyun.com/articles/655109
https://blog.csdn.net/lilong117194/article/details/78111779
作为优化方法时,利用了二阶导数的信息,因此下降更快。
牛顿法的优缺点总结：
　　优点：二阶收敛，收敛速度快；
　　缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵(二阶导逆矩阵)，计算比较复杂。H矩阵半正定,可能没有逆,求解过程无法持续。
XGBoost本质上就是利用牛顿法进行优化的


拟牛顿法:求解无约束最优化。常用的拟牛顿法有DFP算法和BFGS算法。
拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。

凸优化
http://www.cnblogs.com/tornadomeet/p/3300132.html
凸集，凸函数，凸优化问题，线性规划，二次规划，二次约束二次规划，半正定规划
凸优化定义：目标函数为凸函数,不等式函数为凸函数，等式函数为仿射函数。




梯度下降 ,最小二乘法/牛顿法/拟牛顿法/共轭梯度法/GIS/IIS
  协方差矩阵
