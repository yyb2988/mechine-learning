熵,条件熵,最大熵,联合熵,相对熵,交叉熵,互信息
准确率 Accuracy,召回率 Recall,F1值

auc值：x代表假真率,y代表真正率;原点代表全预测为负,(1,1)点代表全预测为正数。

线性回归 LR,均方误差 MSE
ε(i)∼N(0, σ2) 艾普西隆,误差最大似然函数求极大,推导出最小二乘法。

判别模型：没有对p(x,y)的建模,没有对x本身的建模,不考虑X是怎么来的！只关注p(y|x)
逻辑回归,最大熵模型,决策树(随机森林),SVM
生成模型：对p(x,y)建模,会考虑x是怎么来的!可能会关注p(x|y)
素贝叶斯,EM算法, LDA,PLSA,HMM, 高斯混合GMM分布(可以认为是素朴贝叶斯的离散形式) ,隐形马尔科夫模型


LR 逻辑回归
似然函数 || h(xi)的yi次方 * （1-h(xi)）的（1-yi）次方
对于y=1,y=0的样本,似然概率分别为h(xi) , 1-h(xi)
损失函数,对似然函数求log后,再乘以-1
-1 * 所有样本求和 yi*log(h(xi)) + (1-yi)log(1-h(xi))
这不就是交叉熵函数吗？


SVM彻底解释版:
https://blog.csdn.net/b285795298/article/details/81977271
点到面的距离  |wx+b| /||w||
令支持向量点 |wx+b| = 1, 即最大化2/||w||,最小化2||w||平方,约束条件y(i)*(wx+b) >= 1  
=> 1 -y(i)*(wx+b)  <= 0
损失函数自带L2正则项,所以也是结构风险最小化的根本原因;
凸二次规划：凸函数：求最小值，所以是凸优化;目标是二次的，约束为线性的,所以二次;
拉格朗日乘子法,得到“对偶问题”,消除不等式约束。
拉格朗日函数：L(W,B,a) = 0.5||w||平方 + 对每个I a(i)*(1-yi*(wxi + b)) 
其中a（i）>= 0
只有满足了KKT条件,才能有等价的对偶问题： min(w,b) max(a) L(W,B,A) 对偶于 max(a) min(w,b) L(W,B,A)
对于一般优化问题，满足KKT条件是min[max(f)]=max[min(f)]的必要条件。但SVM中约束条件是线性的，目标函数是二次的，对于这样的凸优化问题，KKT条件是充分的。
KKT条件：KKT条件就是函数的最优值必须满足下面条件：(KKT条件将在SMO算法中起到作用)
min f(x);g(x) = 0;h(x) <=0 
l(x) = f(x) + a*g(x) + b*h(x)
条件1： l(x)对x导数为0
条件2： 不等式约束h(x) <=0 ,并且 b*h(x) = 0
条件3： 等式约束g(x) = 0
条件4： b >= 0
（强对偶条件）
min(x) max(a,b) l(x) => max(a,b) min(x) l(x)
使用拉格朗日乘子法将原问题转化为对偶问题求解。
具体做法是：
（1）将约束融入目标函数中，得到拉格朗日函数
（2）然后对模型参数w和b求偏导，并令之为零:
    w = 所有样本求和 ：a（i） * yi * xi  【每个样本都会有一个yi,一个ai】
    所有样本求和a(i) * yi = 0
（3）得到w 后，将其带入拉格朗日函数中，消去模型参数w和b
    此时的公式为 L(a) = - 0.5 * 所有样本I求和 { 所有样本J求和： a(i) * a(j) * x(i) * x(j) * y(i) *y(j)  }  +  所有样本I求和：a(i)
    【从这个公式,就能方便地引入核函数,也是求对偶问题的一个很大原因】
（4）这样就得到了原问题的对偶问题，对偶问题和原问题等价，同时对偶问题也是一个凸优化问题，使用SMO算法求解拉格朗日乘子a
    此时凸优化问题变成了：max L(A) => min - L(a)
    最小化 - L(a) =  0.5 * 所有样本I求和 { 所有样本J求和： a(i) * a(j) * x(i) * x(j) * y(i) *y(j)  }  -  所有样本I求和：a(i)
    约束条件1：所有样本求和a(i) * yi = 0
    还有原问题的KKT约束条件：a(i)*(1-yi*(wxi + b)) = 0 ; a(i) >= 0
    SMO极好文章：https://blog.csdn.net/jesmine_gu/article/details/84024702
                http://blog.sina.com.cn/s/blog_62970c250102xfok.html
（5）得到拉格朗日乘子后，进一步可以得到模型参数w和b，也就得到了我们想要的划分超平面
    w = 所有样本求和 ：a（i） * yi * xi  【每个样本都会有一个y,一个a】
    b的求法:取一个a(j)>0 如果有松弛变量和惩罚因子C,a(j)还要小于C【此时对应的样本为支持向量,等于0的为不需要松弛的样本】
           b = y(j) - 所有样本i求和 a(i)*y(i)* K(xi,xj)
-----------------------------------------------------------------------------------
线性不可分支持向量机：
增加松弛变量 s(i),惩罚因子C
约束变为：y(i)*(wx+b) >= 1 -s(i)  和  s(i) >= 0
在拉格朗日阶段,a(i)约束变为  0 =< a(i) <= C
a=0的是不需要松弛的样本，离间隔边界很远,也可能极少数在边界上
a > 0 并且 a < C的,都在间隔边界上(分类OK)
a=C , 跳出了间隔边界的点,也可能极少数在边界上
软间隔SVM极好的文章：https://blog.csdn.net/robin_xu_shuai/article/details/77051258
------------------------------------------------------------------------------------
核函数：
维空间向高维空间映射后,再分;但是核函数可以在低维度空间直接计算
线性核函数：k(x1,x2) = x1*x2
径向基(高斯)核函数：k(x1,x2) = e -(|x1-x2|平方/2*σ平方);西格玛越大,分类越拟好。gamma=1/(2*sigma^2)
                  参数gamma越大,分类越拟合。
多项式核函数：k(x1,x2) = (x1*x2+d)的m次方 



最大熵:
https://www.cnblogs.com/wxquare/p/5858008.html 


CRF
轻松理解版本：https://www.imooc.com/article/27795（与HMM类比）
HMM可以认为是CRF的特例


三种估计的区别:最大似然  最大后验 贝叶斯估计：
https://www.cnblogs.com/shixisheng/p/7136890.html
最大似然p(x|0)是频率学派,认为参数天生固定,最大化求解就可以了。例如: plsa,lr
其他是贝叶斯学派，认为参数也有分布；最大后验：朴素贝叶斯公式；贝叶斯估计例如lda
狄利克雷分布是多项式分布的共轭先验
beta分布是二项分布的共轭先验
【先验概率】叫做【似然概率】的共轭先验 、 参数的【后验概率与先验概率具有相同的形式】

牛顿法：
https://yq.aliyun.com/articles/655109
https://blog.csdn.net/lilong117194/article/details/78111779

统计学习方法精华笔记：
https://www.cnblogs.com/limitlessun/p/8611103.html

knn/kd树：
https://blog.csdn.net/wzgang123/article/details/20542209

混合高斯GMM和EM最好文章:
https://zhuanlan.zhihu.com/p/44673577
EM算法(隐马学习Baum Welch)

HMM(隐马尔科夫模型):
https://www.cnblogs.com/YongSun/p/4767667.html
概率计算问题(暴力：(2T-1)*N^T ；前向算法-后向算法T*N^2) 
学习问题(baum welch) ,EM算法Q函数  对每个I求和： p(I|X,0) log (p(I,X|0))
预测(解码)问题(：T*N^T暴力; 动态规划-维特比算法T*N^2)

PCA数学原理:
https://www.cnblogs.com/mikewolf2002/p/3429711.html
目标：转换后的协方差矩阵对角线从大到小;非对角线全是0(方差最大,协方差为零)
P为新坐标基
D = y (y转置) = （px）(px)转置 =p*x*(x转置)*(p转置) = p*(x的协方差矩阵)*P转置
转化为了x协方差矩阵的对角化问题;


SVD-奇异值分解/特征值分解
https://blog.csdn.net/lipengcn/article/details/51992766
https://blog.csdn.net/lyf52010/article/details/79942966
(最早的应用,信息检索)LSA:（10000文档*2000词 ：）  10000*10000  (10000*2000) 2000*2000  
     降维后: 10000文档*100 (100*100) 100*2000 
     左奇异矩阵U、sigma矩阵Σ、右奇异矩阵VT

皮尔逊相关系数(相当于x,y中心化后的余弦相似度)
(x-x平均)*(y-y平均) /  (x-x平均）向量的模 *  (y-y平均）向量的模

plsa/em算法/GMM混合高斯模型
https://www.cnblogs.com/zjgtan/p/3887132.html

=======================================================================================================================
决策树(ID3,C4.5,CART-分类回归树)
ID3，C4.5 只做分类 ; CART 可以做分类和回归
ID3不能处理连续值,缺失值。IG信息增益作为特征选择依据,条件熵的减少量
C4.5可以处理连续值,但是需要扫描数据寻找分割点;IGR信息增益率作为特征选择依据,克服对于多值属性的倾向。
C4.5采用PEP悲观剪枝法，它使用训练集生成决策树又用它来进行剪枝，不需要独立的剪枝集。
C4.5处理缺失值:https://www.cnblogs.com/starfire86/p/5791264.html
CART分类和回归树：分类时用GINI系数，求和[1-p2],和熵一样,越大越混杂;回归时,用平方误差即可。
CART剪枝采用代价复杂度
预剪枝方法：定义层高,定义信息增益减少量,定义样本数
后剪枝方法：https://blog.csdn.net/weixin_43216017/article/details/87534496
Reduced-Error Pruning(REP,错误率降低剪枝)
Pesimistic-Error Pruning(PEP,悲观错误剪枝）
Cost-Complexity Pruning（CCP，代价复杂度剪枝)

bagging,boosting区别:
https://www.cnblogs.com/earendil/p/8872001.html

随机森林,特征重要度+sklearn例子：
https://blog.csdn.net/zjuPeco/article/details/77371645?locationNum=7&fps=1

adboost/gbdt/xgboost
https://blog.csdn.net/shine19930820/article/details/65633436
https://www.cnblogs.com/zhouxiaohui888/p/6008368.html
=========================================================================================================================

sequence to sequence 模型
cnn
text cnn
rnn lstm 双向lstm
https://www.cnblogs.com/wushaogui/p/9176617.html

  逻辑回归
  决策树(ID3,C4.5,CART-分类回归树)
  线性回归，svm(正定核) ， 决策树 ， 隐马(HMM) ， 条件随机场(CRF)  , 最大熵(拉格朗日乘子/对偶相关理论) ,海塞矩阵(半正定,矩阵相关理论)
  贝叶斯网络 ， LDA/PLSA , 神经网络 ， 深度学习 ， numpy / skelearn / keras / tensorflow , 梯度下降 ,最小二乘法/牛顿法/拟牛顿法/共轭梯度法/GIS/IIS
  协方差矩阵
  各类距离度量: 明氏距离(曼哈顿距离,欧式距离,切比雪夫距离),皮尔逊相关系数,马氏距离,汉明距离
  knn/kd树
  朴素贝叶斯/文本分类(伯努利-多项式)
   , cnn , rnn , lstm , EM算法 , GMM混合高斯模型 , K-Means , 层次聚类, KNN(KD树)
  推荐系统系列
  以下的各类关系和公式推导：互信息,联合熵,条件熵,边缘熵,熵(经验熵，香农熵)
  泊松分布,伽马分布,LDA-EM算法,马氏链/吉布斯采样
  特征选择:卡方CHI, 互信息(信息增益), Random Forest
  矩阵系列：PCA / (LSA LSI ) SVD / SVD++ / 矩阵分解 / MF / FM
  集成学习bagging:  代表算法随机森林 RF。样本有放回,特征无放回,无剪枝,各个模型均匀投票 ;调参
  集成学习boosting: 代表算法梯度提升数(GBDT), Adaboost, Xgboost
  集成学习stacking:


  矩阵专题：
  正交化 / 单位矩阵 / 转置
