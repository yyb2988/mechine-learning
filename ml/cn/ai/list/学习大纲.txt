熵,条件熵,最大熵,联合熵,相对熵,交叉熵,互信息
准确率 Accuracy,召回率 Recall,F1值
auc值：x代表假真率,y代表真正率;
线性回归 LR,均方误差 MSE

判别模型：没有对p(x,y)的建模,没有对x本身的建模,不考虑X是怎么来的！只关注p(y|x)
逻辑回归,最大熵模型,决策树(随机森林),SVM
生成模型：对p(x,y)建模,会考虑x是怎么来的!可能会关注p(x|y)
素贝叶斯,EM算法, LDA,HMM, 高斯混合GMM分布(可以认为是素朴贝叶斯的离散形式) ,隐形马尔科夫模型


LR 逻辑回归
似然函数 || h(xi)的yi次方 * （1-h(xi)）的（1-yi）次方
对于y=1,y=0的样本,似然概率分别为h(xi) , 1-h(xi)
损失函数,对似然函数求log后,再乘以-1
-1 * 所有样本求和 yi*log(h(xi)) + (1-yi)log(1-h(xi))
这不就是交叉熵函数吗？


SVM彻底解释版:
https://blog.csdn.net/b285795298/article/details/81977271
点到面的距离  |wx+b| /||w||
令支持向量点 |wx+b| = 1, 即最大化2/||w||,最小化2||w||平方,约束条件y(i)*(wx+b) >= 1
凸二次规划：凸函数：求最小值，所以是凸优化;目标是二次的，约束为线性的,所以二次;
拉格朗日乘子法,得到“对偶问题”,消除不等式约束。
拉格朗日函数：L(W,B,a) = 0.5||w||平方 + 对每个I a(i)*(1-yi*(wxi + b)) 
其中a（i）>= 0
只有满足了KKT条件,才能有等价的对偶问题： min(w,b) max(a) L(W,B,A) 对偶于 max(a) min(w,b) L(W,B,A)
使用拉格朗日乘子法将原问题转化为对偶问题求解。
具体做法是：
（1）将约束融入目标函数中，得到拉格朗日函数
（2）然后对模型参数w和b求偏导，并令之为零:
    w = 所有样本求和 ：a（i） * yi * xi  【每个样本都会有一个yi,一个ai】
    所有样本求和a(i) * yi = 0
（3）得到w 后，将其带入拉格朗日函数中，消去模型参数w和b
    此时的公式为 L(a) = - 0.5 * 所有样本I求和 { 所有样本J求和： a(i) * a(j) * x(i) * x(j) * y(i) *y(j)  }  +  所有样本I求和：a(i)
    【从这个公式,就能方便地引入核函数,也是求对偶问题的一个很大原因】
（4）这样就得到了原问题的对偶问题，对偶问题和原问题等价，同时对偶问题也是一个凸优化问题，使用SMO算法求解拉格朗日乘子a
    此时凸优化问题变成了：max L(A) => min - L(a)
    最小化 - L(a) =  0.5 * 所有样本I求和 { 所有样本J求和： a(i) * a(j) * x(i) * x(j) * y(i) *y(j)  }  -  所有样本I求和：a(i)
    约束条件1：所有样本求和a(i) * yi = 0
    约束条件2：所有样本 a(i) >= 0
    
（5）得到拉格朗日乘子后，进一步可以得到模型参数w和b，也就得到了我们想要的划分超平面
    w = 所有样本求和 ：a（i） * yi * xi  【每个样本都会有一个y,一个a】
    b的求法:正样本里面,离分类面最近的点；负样本里面,离分类面最近的点；两个点之间的距离为2即可
           正样本里面，离分类面最近的点,即wx最小的那个,为x1
           正样本里面，离分类面最近的点,即wx最大的那个,为x0
           令wx1 + b = 1,wx0 + b = -1  两式相加 =>  b = 1(min(wx0+wx1)/2







批量梯度下降(BGD),随机梯度下降(SGD),小批量梯度下降(MBGD)：
https://www.cnblogs.com/lliuye/p/9451903.html

梯度下降为什么陡:
https://www.zhihu.com/question/36301367/answer/198887937

最大熵:
https://www.cnblogs.com/wxquare/p/5858008.html

三种估计的区别:最大似然  最大后验 贝叶斯估计：
https://www.cnblogs.com/shixisheng/p/7136890.html
最大似然是频率学派,认为参数天生固定;例如plsa
其他是贝叶斯学派，认为参数也有分布；最大后验：朴素贝叶斯公式；贝叶斯估计例如lda
狄利克雷分布是多项式分布的共轭先验
beta分布是二项分布的共轭先验
先验概率叫做似然概率的共轭先验 、 参数的后验概率与先验概率具有相同的形式

牛顿法：
https://yq.aliyun.com/articles/655109
https://blog.csdn.net/lilong117194/article/details/78111779

统计学习方法精华笔记：
https://www.cnblogs.com/limitlessun/p/8611103.html

knn/kd树：
https://blog.csdn.net/wzgang123/article/details/20542209

混合高斯GMM和EM最好文章:
https://zhuanlan.zhihu.com/p/44673577
EM算法(隐马学习Baum Welch)

HMM(隐马尔科夫模型):
预测问题(暴力：T*N^T ；前向算法-后向算法T*N^2) 
学习问题(baum welch) ,EM算法Q函数  对每个I求和： p(I|X,0) log (p(I,X|0))
解码问题(：T*N^T暴力; 动态规划-维特比算法T*N^2)

PCA数学原理:
https://www.cnblogs.com/mikewolf2002/p/3429711.html
目标：转换后的协方差矩阵对角线从大到小;非对角线全是0(方差最大,协方差为零)
P为新坐标基
D = y (y转置) = （px）(px)转置 =p*x*(x转置)*(p转置) = p*(x的协方差矩阵)*P转置
转化为了x协方差矩阵的对角化问题;


SVD-奇异值分解/特征值分解
https://blog.csdn.net/lipengcn/article/details/51992766
https://blog.csdn.net/lyf52010/article/details/79942966
(最早的应用,信息检索)LSA:（10000文档*2000词 ：）  10000*10000  (10000*2000) 2000*2000  
     降维后: 10000文档*100 (100*100) 100*2000 
     左奇异矩阵U、sigma矩阵Σ、右奇异矩阵VT

皮尔逊相关系数(相当于x,y中心化后的余弦相似度)
(x-x平均)*(y-y平均) /  (x-x平均）向量的模 *  (y-y平均）向量的模


plsa/em算法
https://www.cnblogs.com/ywl925/p/3552815.html

决策树剪枝：悲观pep,代价复杂度ccp
http://blog.sina.com.cn/s/blog_4e4dec6c0101fdz6.html

bagging,boosting区别:
https://www.cnblogs.com/earendil/p/8872001.html

随机森林,特征重要度+sklearn例子：
https://blog.csdn.net/zjuPeco/article/details/77371645?locationNum=7&fps=1

adboost/gbdt/xgboost
https://blog.csdn.net/shine19930820/article/details/65633436
https://www.cnblogs.com/zhouxiaohui888/p/6008368.html


sequence to sequence 模型
cnn
text cnn
rnn lstm 双向lstm
https://www.cnblogs.com/wushaogui/p/9176617.html


  逻辑回归
  决策树(ID3,C4.5,CART-分类回归树)
  线性回归，svm(正定核) ， 决策树 ， 隐马(HMM) ， 条件随机场(CRF)  , 最大熵(拉格朗日乘子/对偶相关理论) ,海塞矩阵(半正定,矩阵相关理论)
  贝叶斯网络 ， LDA/PLSA , 神经网络 ， 深度学习 ， numpy / skelearn / keras / tensorflow , 梯度下降 ,最小二乘法/牛顿法/拟牛顿法/共轭梯度法/GIS/IIS
  协方差矩阵
  各类距离度量: 明氏距离(曼哈顿距离,欧式距离,切比雪夫距离),皮尔逊相关系数,马氏距离,汉明距离
  knn/kd树
  朴素贝叶斯/文本分类(伯努利-多项式)
   , cnn , rnn , lstm , EM算法 , GMM混合高斯模型 , K-Means , 层次聚类, KNN(KD树)
  推荐系统系列
  以下的各类关系和公式推导：互信息,联合熵,条件熵,边缘熵,熵(经验熵，香农熵)
  泊松分布,伽马分布,LDA-EM算法,马氏链/吉布斯采样
  特征选择:卡方CHI, 互信息(信息增益), Random Forest
  矩阵系列：PCA / (LSA LSI ) SVD / SVD++ / 矩阵分解 / MF / FM
  集成学习bagging:  代表算法随机森林 RF。样本有放回,特征无放回,无剪枝,各个模型均匀投票 ;调参
  集成学习boosting: 代表算法梯度提升数(GBDT), Adaboost, Xgboost
  集成学习stacking:


  矩阵专题：
  正交化 / 单位矩阵 / 转置
