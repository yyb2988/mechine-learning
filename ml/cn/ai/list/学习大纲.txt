熵,条件熵,最大熵,联合熵,相对熵,互信息
准确率 Accuracy,召回率 Recall,F1值
线性回归 LR,均方误差 MSE

批量梯度下降(BGD),随机梯度下降(SGD),小批量梯度下降(MBGD)：
https://www.cnblogs.com/lliuye/p/9451903.html

梯度下降为什么陡:
https://www.zhihu.com/question/36301367/answer/198887937

最大熵:
https://www.cnblogs.com/wxquare/p/5858008.html

三种估计的区别:最大似然  最大后验 贝叶斯估计：
https://www.cnblogs.com/shixisheng/p/7136890.html

牛顿法：
https://yq.aliyun.com/articles/655109
https://blog.csdn.net/lilong117194/article/details/78111779

统计学习方法精华笔记：
https://www.cnblogs.com/limitlessun/p/8611103.html

knn/kd树：
https://blog.csdn.net/wzgang123/article/details/20542209

混合高斯GMM和EM最好文章:
https://zhuanlan.zhihu.com/p/44673577

PCA数学原理:
https://www.cnblogs.com/mikewolf2002/p/3429711.html

plsa/em算法
https://www.cnblogs.com/ywl925/p/3552815.html

决策树剪枝：悲观pep,代价复杂度ccp
http://blog.sina.com.cn/s/blog_4e4dec6c0101fdz6.html

bagging,boosting区别:
https://www.cnblogs.com/earendil/p/8872001.html

随机森林,特征重要度+sklearn例子：
https://blog.csdn.net/zjuPeco/article/details/77371645?locationNum=7&fps=1

sequence to sequence 模型
cnn
text cnn
rnn lstm 双向lstm
https://www.cnblogs.com/wushaogui/p/9176617.html


  逻辑回归
  决策树(ID3,C4.5,CART-分类回归树)
  线性回归，svm(正定核) ， 决策树 ， 隐马(HMM) ， 条件随机场(CRF)  , 最大熵(拉格朗日乘子/对偶相关理论) ,海塞矩阵(半正定,矩阵相关理论)
  贝叶斯网络 ， LDA/PLSA , 神经网络 ， 深度学习 ， numpy / skelearn / keras / tensorflow , 梯度下降 ,最小二乘法/牛顿法/拟牛顿法/共轭梯度法/GIS/IIS
  协方差矩阵
  各类距离度量: 明氏距离(曼哈顿距离,欧式距离,切比雪夫距离),皮尔逊相关系数
  knn/kd树
  朴素贝叶斯/文本分类(伯努利-多项式)
   , cnn , rnn , lstm , EM算法 , GMM混合高斯模型 , K-Means , 层次聚类, KNN(KD树)
  推荐系统系列 , xgboost
  以下的各类关系和公式推导：互信息,联合熵,条件熵,边缘熵,熵(经验熵，香农熵)
  泊松分布,伽马分布,LDA-EM算法,马氏链/吉布斯采样
  特征选择:卡方, KL离散度
  PCA / SVD / SVD++ / 矩阵分解 / MF / FM
  集成学习bagging:  代表算法随机森林 RF。样本有放回,特征无放回,无剪枝,各个模型均匀投票 ;调参
  集成学习boosting: 代表算法梯度提升数(GBDT),Adaboost
  集成学习stacking:


